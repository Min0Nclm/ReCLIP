model_name: ViT-L-14
layers_out: [12, 18, 24]
data_root: ./data
train_dataset: brainmri
test_datasets:
  - brainmri
image_size: 288 # Using 288 as in deco_diff examples
batch_size: 8
epoch: 100
lr: 0.0001
print_freq_step: 100
val_freq_epoch: 1
save_root: ./results
random_seed: 100

# New parameters for integrated model
w1: 1.0 # Weight for MediCLIP loss
w2: 1.0 # Weight for DecoDiff loss

mask_ratio: 0.7 # Ratio of latent space to mask
mask_patch_size: 1 # Patch size for masking (1 for pixel-wise)

# MedVAE configuration
medvae_model_name: medvae_4_1_2d # Example: 2D, 4x compression, 1 channel latent
medvae_modality: xray # Example: Modality for MedVAE

# Latent space dimensions (derived from image_size and VAE compression)
# For medvae_4_1_2d, 4x compression means latent_size = image_size / 4
latent_size: 72 # 288 / 4 = 72
embed_dim: 4 # Channel dimension of MedVAE latent

# DecoDiff UNet configuration
deco_diff_model_size: UNet_L # Size of the UNet for deviation prediction

# Adapter and Projection Head output dimension (CLIP embedding dimension)
layers_out_adapter: 512 # Common CLIP embedding dimension
